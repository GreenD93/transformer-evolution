{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfpg-n2gNgtB",
        "colab_type": "text"
      },
      "source": [
        "## BERT 구현 과정 (2/2)\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2020-01-02/bert-classification.png)\n",
        "\n",
        "BERT 모델 구현에 대한 설명 입니다.\n",
        "\n",
        "이 내용을 확인하기 전 아래 내용을 확인하시기 바랍니다.\n",
        "- [Sentencepiece를 활용해 Vocab 만들기](https://paul-hyun.github.io/vocab-with-sentencepiece/)\n",
        "- [Naver 영화리뷰 감정분석 데이터 전처리 하기](https://paul-hyun.github.io/preprocess-nsmc/)\n",
        "- [Transformer (Attention Is All You Need) 구현하기 (1/3)](https://paul-hyun.github.io/transformer-01/)\n",
        "- [Transformer (Attention Is All You Need) 구현하기 (2/3)](https://paul-hyun.github.io/transformer-02/)\n",
        "- [Transformer (Attention Is All You Need) 구현하기 (3/3)](https://paul-hyun.github.io/transformer-03/)\n",
        "- [BERT(Bidirectional Encoder Representations from Transformers) 구현하기 (1/2)](https://paul-hyun.github.io/bert-01/)\n",
        "\n",
        "[Colab](https://colab.research.google.com/)에서 실행 했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4fLocKzS8qH",
        "colab_type": "text"
      },
      "source": [
        "#### 0. Pip Install\n",
        "필요한 패키지를 pip를 이용해서 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP4qW5w6TAXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "680abb5b-6750-4eab-c9c5-3c8128ef9141"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=ad5b0abc9c630f867606ad1571d31fd2bf035835c8eb91f96f6ca60d61f651fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZs93qCwS_bM",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Google Drive Mount\n",
        "Colab에서는 컴퓨터에 자원에 접근이 불가능 하므로 Google Drive에 파일을 올려 놓은 후 Google Drive를 mount 에서 로컬 디스크처럼 사용 합니다.\n",
        "1. 아래 블럭을 실행하면 나타나는 링크를 클릭하세요.\n",
        "2. Google 계정을 선택 하시고 허용을 누르면 나타나는 코드를 복사하여 아래 박스에 입력한 후 Enter 키를 입력하면 됩니다.\n",
        "\n",
        "학습관련 [데이터 및 결과 파일](https://drive.google.com/open?id=15XGr-L-W6DSoR5TbniPMJASPsA0IDTiN)을 참고 하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XR4LcDdNfnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b4898ae7-39ec-40d5-fd93-9c945c2c23cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# data를 저장할 폴더 입니다. 환경에 맞게 수정 하세요.\n",
        "data_dir = \"/content/drive/My Drive/Data/transformer-evolution\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMgpP6fjTJF8",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRgT80wpTJiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm, tqdm_notebook, trange\n",
        "import sentencepiece as spm\n",
        "import wget\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRrdaSJ_TNAf",
        "colab_type": "text"
      },
      "source": [
        "#### 3. 폴더의 목록을 확인\n",
        "Google Drive mount가 잘 되었는지 확인하기 위해 data_dir 목록을 확인 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfWB9L0_TQlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1a2370d6-ade6-4676-fae8-57b0e5c62efe"
      },
      "source": [
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kowiki.csv.gz\n",
            "kowiki.model\n",
            "kowiki.vocab\n",
            "ratings_train.txt\n",
            "ratings_test.txt\n",
            "ratings_train.json\n",
            "ratings_test.json\n",
            "kowiki.txt\n",
            "kowiki_gpt.json\n",
            "save_gpt_pretrain.pth\n",
            "kowiki_bert_0.json\n",
            "save_bert_pretrain.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUOwhKMyTXNQ",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Vocab 및 입력\n",
        "\n",
        "[Sentencepiece를 활용해 Vocab 만들기](https://paul-hyun.github.io/vocab-with-sentencepiece/)를 통해 만들어 놓은 vocab을 로딩 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LX6VgIkTaKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26b7ef91-b386-4766-e966-1f22ca91507b"
      },
      "source": [
        "# vocab loading\n",
        "vocab_file = f\"{data_dir}/kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgziU4ATcyN",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Config\n",
        "\n",
        "모델에 설정 값을 전달하기 위한 config를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDcRg9V0Tdc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
        "class Config(dict): \n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUztpEf6Tfx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e1077d9-a169-4294-b343-ac51c61fed8d"
      },
      "source": [
        "config = Config({\n",
        "    \"n_enc_vocab\": len(vocab),\n",
        "    \"n_enc_seq\": 256,\n",
        "    \"n_seg_type\": 2,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidn\": 256,\n",
        "    \"i_pad\": 0,\n",
        "    \"d_ff\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})\n",
        "print(config)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_enc_vocab': 8007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j93X24LtTijG",
        "colab_type": "text"
      },
      "source": [
        "#### 6. BERT\n",
        "\n",
        "BERT Class 및 함수 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_41WubQUImx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" sinusoid position encoding \"\"\"\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
        "    def cal_angle(position, i_hidn):\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "\"\"\" attention pad mask \"\"\"\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>\n",
        "    return pad_attn_mask\n",
        "\n",
        "\n",
        "\"\"\" attention decoder mask \"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
        "    return subsequent_mask\n",
        "\n",
        "\n",
        "\"\"\" scale dot product attention \"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        attn_prob = self.dropout(attn_prob)\n",
        "        # (bs, n_head, n_q_seq, d_v)\n",
        "        context = torch.matmul(attn_prob, V)\n",
        "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
        "        return context, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" multi head attention \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
        "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "        # (bs, n_head, n_q_seq, d_head)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_k_seq, d_head)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_v_seq, d_head)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
        "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
        "        # (bs, n_head, n_q_seq, e_embd)\n",
        "        output = self.linear(context)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        return output, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" feed forward \"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (bs, d_ff, n_seq)\n",
        "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDXUeMKoULa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" encoder layer \"\"\"\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(self.config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "    \n",
        "    def forward(self, inputs, attn_mask):\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
        "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
        "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
        "        # (bs, n_enc_seq, d_hidn)\n",
        "        ffn_outputs = self.pos_ffn(att_outputs)\n",
        "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
        "        return ffn_outputs, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" encoder \"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
        "        self.pos_emb = nn.Embedding(self.config.n_enc_seq + 1, self.config.d_hidn)\n",
        "        self.seg_emb = nn.Embedding(self.config.n_seg_type, self.config.d_hidn)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
        "        pos_mask = inputs.eq(self.config.i_pad)\n",
        "        positions.masked_fill_(pos_mask, 0)\n",
        "\n",
        "        # (bs, n_enc_seq, d_hidn)\n",
        "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)  + self.seg_emb(segments)\n",
        "\n",
        "        # (bs, n_enc_seq, n_enc_seq)\n",
        "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
        "\n",
        "        attn_probs = []\n",
        "        for layer in self.layers:\n",
        "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
        "            outputs, attn_prob = layer(outputs, attn_mask)\n",
        "            attn_probs.append(attn_prob)\n",
        "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return outputs, attn_probs\n",
        "\n",
        "\n",
        "\"\"\" bert \"\"\"\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.encoder = Encoder(self.config)\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        # (bs, n_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
        "        # (bs, n_enc_seq, n_enc_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return outputs, self_attn_probs\n",
        "    \n",
        "    def save(self, epoch, loss, path):\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": loss,\n",
        "            \"state_dict\": self.state_dict()\n",
        "        }, path)\n",
        "    \n",
        "    def load(self, path):\n",
        "        save = torch.load(path)\n",
        "        self.load_state_dict(save[\"state_dict\"])\n",
        "        return save[\"epoch\"], save[\"loss\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZQyVfnJUcXH",
        "colab_type": "text"
      },
      "source": [
        "#### 7. Naver 영화 분류 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIu5UyLUUdMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" naver movie classfication \"\"\"\n",
        "class MovieClassification(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BERT(self.config)\n",
        "        # classfier\n",
        "        self.projection_cls = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
        "    \n",
        "    def forward(self, inputs, segments):\n",
        "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        output, attn_probs = self.bert(inputs, segments)\n",
        "        # (bs, d_hidn)\n",
        "        output = output[:, 0].contiguous()\n",
        "        # (bs, n_output)\n",
        "        logits_cls = self.projection_cls(output)\n",
        "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
        "        return logits_cls, attn_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_DfRSm3Uzg3",
        "colab_type": "text"
      },
      "source": [
        "#### 8. 네이버 영화 분류 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F8q2PatU0KH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 영화 분류 데이터셋 \"\"\"\n",
        "class MovieDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, infile):\n",
        "        self.vocab = vocab\n",
        "        self.labels = []\n",
        "        self.sentences = []\n",
        "        self.segments = []\n",
        "\n",
        "        line_cnt = 0\n",
        "        with open(infile, \"r\") as f:\n",
        "            for line in f:\n",
        "                line_cnt += 1\n",
        "\n",
        "        with open(infile, \"r\") as f:\n",
        "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=\"Loading Dataset\", unit=\" lines\")):\n",
        "                data = json.loads(line)\n",
        "                self.labels.append(data[\"label\"])\n",
        "                sentence = [vocab.piece_to_id(\"[CLS]\")] + [vocab.piece_to_id(p) for p in data[\"doc\"]] + [vocab.piece_to_id(\"[SEP]\")]\n",
        "                self.sentences.append(sentence)\n",
        "                self.segments.append([0] * len(sentence))\n",
        "    \n",
        "    def __len__(self):\n",
        "        assert len(self.labels) == len(self.sentences)\n",
        "        assert len(self.labels) == len(self.segments)\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return (torch.tensor(self.labels[item]),\n",
        "                torch.tensor(self.sentences[item]),\n",
        "                torch.tensor(self.segments[item]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIUF3NDiU9JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" movie data collate_fn \"\"\"\n",
        "def movie_collate_fn(inputs):\n",
        "    labels, inputs, segments = list(zip(*inputs))\n",
        "\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
        "\n",
        "    batch = [\n",
        "        torch.stack(labels, dim=0),\n",
        "        inputs,\n",
        "        segments,\n",
        "    ]\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7h4x5WAVE-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "73653535-171b-4970-c6b1-5d6c25f0076d"
      },
      "source": [
        "\"\"\" 데이터 로더 \"\"\"\n",
        "batch_size = 128\n",
        "train_dataset = MovieDataSet(vocab, f\"{data_dir}/ratings_train.json\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=movie_collate_fn)\n",
        "test_dataset = MovieDataSet(vocab, f\"{data_dir}/ratings_test.json\")\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Dataset: 100%|██████████| 149995/149995 [00:04<00:00, 30714.39 lines/s]\n",
            "Loading Dataset: 100%|██████████| 49997/49997 [00:01<00:00, 29352.39 lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ROkwVAZVMBq",
        "colab_type": "text"
      },
      "source": [
        "#### 9. 네이버 영화 분류 데이터 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uk8i_COVMqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 모델 epoch 평가 \"\"\"\n",
        "def eval_epoch(config, model, data_loader):\n",
        "    matchs = []\n",
        "    model.eval()\n",
        "\n",
        "    n_word_total = 0\n",
        "    n_correct_total = 0\n",
        "    with tqdm(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
        "        for i, value in enumerate(data_loader):\n",
        "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "            outputs = model(inputs, segments)\n",
        "            logits_cls = outputs[0]\n",
        "            _, indices = logits_cls.max(1)\n",
        "\n",
        "            match = torch.eq(indices, labels).detach()\n",
        "            matchs.extend(match.cpu())\n",
        "            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Acc: {accuracy:.3f}\")\n",
        "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDuh_wmmVWSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 모델 epoch 학습 \"\"\"\n",
        "def train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader):\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "        for i, value in enumerate(train_loader):\n",
        "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, segments)\n",
        "            logits_cls = outputs[0]\n",
        "\n",
        "            loss_cls = criterion_cls(logits_cls, labels)\n",
        "            loss = loss_cls\n",
        "\n",
        "            loss_val = loss_cls.item()\n",
        "            losses.append(loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
        "    return np.mean(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8zkUfOmVjNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "148fb0cf-91fd-4c0c-bb02-ee4671c1869b"
      },
      "source": [
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "config.n_output = 2\n",
        "print(config)\n",
        "\n",
        "learning_rate = 5e-5\n",
        "n_epoch = 10"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_enc_vocab': 8007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cuda'), 'n_output': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wZhTd_zVo8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model):\n",
        "    model.to(config.device)\n",
        "\n",
        "    criterion_cls = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_epoch, best_loss, best_score = 0, 0, 0\n",
        "    losses, scores = [], []\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader)\n",
        "        score = eval_epoch(config, model, test_loader)\n",
        "\n",
        "        losses.append(loss)\n",
        "        scores.append(score)\n",
        "\n",
        "        if best_score < score:\n",
        "            best_epoch, best_loss, best_score = epoch, loss, score\n",
        "    print(f\">>>> epoch={best_epoch}, loss={best_loss:.5f}, socre={best_score:.5f}\")\n",
        "    return losses, scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui2zgSlUVxDw",
        "colab_type": "text"
      },
      "source": [
        "###### Pretrain 없이 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojS0xVsVv_y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "080629af-47d4-42c3-de1c-7bc315c9b9f4"
      },
      "source": [
        "model = MovieClassification(config)\n",
        "\n",
        "losses_00, scores_00 = train(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train(0): 100%|██████████| 1172/1172 [03:31<00:00,  5.81it/s, Loss: 0.367 (0.523)]\n",
            "Valid: 100%|██████████| 391/391 [00:48<00:00,  5.51it/s, Acc: 0.770]\n",
            "Train(1): 100%|██████████| 1172/1172 [03:31<00:00,  5.78it/s, Loss: 0.500 (0.433)]\n",
            "Valid: 100%|██████████| 391/391 [00:48<00:00,  5.62it/s, Acc: 0.770]\n",
            "Train(2): 100%|██████████| 1172/1172 [03:31<00:00,  5.90it/s, Loss: 0.486 (0.408)]\n",
            "Valid: 100%|██████████| 391/391 [00:48<00:00,  5.68it/s, Acc: 0.804]\n",
            "Train(3): 100%|██████████| 1172/1172 [03:31<00:00,  5.89it/s, Loss: 0.441 (0.388)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.88it/s, Acc: 0.805]\n",
            "Train(4): 100%|██████████| 1172/1172 [03:29<00:00,  6.01it/s, Loss: 0.433 (0.373)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.93it/s, Acc: 0.815]\n",
            "Train(5): 100%|██████████| 1172/1172 [03:29<00:00,  5.71it/s, Loss: 0.331 (0.357)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.94it/s, Acc: 0.814]\n",
            "Train(6): 100%|██████████| 1172/1172 [03:29<00:00,  5.76it/s, Loss: 0.415 (0.343)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  5.81it/s, Acc: 0.819]\n",
            "Train(7): 100%|██████████| 1172/1172 [03:29<00:00,  5.96it/s, Loss: 0.340 (0.327)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.94it/s, Acc: 0.818]\n",
            "Train(8): 100%|██████████| 1172/1172 [03:30<00:00,  5.71it/s, Loss: 0.290 (0.309)]\n",
            "Valid: 100%|██████████| 391/391 [00:48<00:00,  5.58it/s, Acc: 0.816]\n",
            "Train(9): 100%|██████████| 1172/1172 [03:30<00:00,  5.88it/s, Loss: 0.291 (0.292)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.86it/s, Acc: 0.814]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>> epoch=6, loss=0.34304, socre=0.81869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJALhcUrV-5T",
        "colab_type": "text"
      },
      "source": [
        "###### Pretrain을 한 후 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGZZyUBV2pI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f1387fb8-c5c2-49af-a452-6da2dc7029bb"
      },
      "source": [
        "model = MovieClassification(config)\n",
        "\n",
        "save_pretrain = f\"{data_dir}/save_bert_pretrain.pth\"\n",
        "model.bert.load(save_pretrain)\n",
        "\n",
        "losses_20, scores_20 = train(model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train(0): 100%|██████████| 1172/1172 [03:30<00:00,  5.67it/s, Loss: 0.406 (0.515)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.97it/s, Acc: 0.784]\n",
            "Train(1): 100%|██████████| 1172/1172 [03:30<00:00,  5.98it/s, Loss: 0.373 (0.434)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  5.89it/s, Acc: 0.796]\n",
            "Train(2): 100%|██████████| 1172/1172 [03:29<00:00,  5.75it/s, Loss: 0.425 (0.403)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.77it/s, Acc: 0.803]\n",
            "Train(3): 100%|██████████| 1172/1172 [03:29<00:00,  5.81it/s, Loss: 0.386 (0.382)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  5.93it/s, Acc: 0.809]\n",
            "Train(4): 100%|██████████| 1172/1172 [03:29<00:00,  6.02it/s, Loss: 0.359 (0.364)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  6.02it/s, Acc: 0.811]\n",
            "Train(5): 100%|██████████| 1172/1172 [03:29<00:00,  5.83it/s, Loss: 0.337 (0.347)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  6.02it/s, Acc: 0.817]\n",
            "Train(6): 100%|██████████| 1172/1172 [03:29<00:00,  5.73it/s, Loss: 0.317 (0.329)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.99it/s, Acc: 0.817]\n",
            "Train(7): 100%|██████████| 1172/1172 [03:29<00:00,  6.04it/s, Loss: 0.291 (0.313)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  5.95it/s, Acc: 0.818]\n",
            "Train(8): 100%|██████████| 1172/1172 [03:30<00:00,  5.49it/s, Loss: 0.294 (0.296)]\n",
            "Valid: 100%|██████████| 391/391 [00:47<00:00,  5.80it/s, Acc: 0.819]\n",
            "Train(9): 100%|██████████| 1172/1172 [03:29<00:00,  5.83it/s, Loss: 0.164 (0.278)]\n",
            "Valid: 100%|██████████| 391/391 [00:46<00:00,  5.92it/s, Acc: 0.819]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>>> epoch=9, loss=0.27761, socre=0.81947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0_DtGYqWCvs",
        "colab_type": "text"
      },
      "source": [
        "#### 10. Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZnPu0tgWIUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "be36a5e8-5d31-48a5-8f05-84d179d922f8"
      },
      "source": [
        "# table\n",
        "data = {\n",
        "    \"loss_00\": losses_00,\n",
        "    \"socre_00\": scores_00,\n",
        "    \"loss_20\": losses_20,\n",
        "    \"socre_20\": scores_20,\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "\n",
        "# graph\n",
        "plt.figure(figsize=[12, 4])\n",
        "plt.plot(scores_00, label=\"score_00\")\n",
        "plt.plot(scores_20, label=\"score_20\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_00</th>\n",
              "      <th>socre_00</th>\n",
              "      <th>loss_20</th>\n",
              "      <th>socre_20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.522875</td>\n",
              "      <td>0.769746</td>\n",
              "      <td>0.515364</td>\n",
              "      <td>0.784407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.433188</td>\n",
              "      <td>0.770006</td>\n",
              "      <td>0.433868</td>\n",
              "      <td>0.796488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.407593</td>\n",
              "      <td>0.804128</td>\n",
              "      <td>0.402953</td>\n",
              "      <td>0.802908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.388438</td>\n",
              "      <td>0.805388</td>\n",
              "      <td>0.382260</td>\n",
              "      <td>0.809349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.372958</td>\n",
              "      <td>0.815309</td>\n",
              "      <td>0.363861</td>\n",
              "      <td>0.810609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.357438</td>\n",
              "      <td>0.814269</td>\n",
              "      <td>0.346687</td>\n",
              "      <td>0.817109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.343040</td>\n",
              "      <td>0.818689</td>\n",
              "      <td>0.329133</td>\n",
              "      <td>0.817189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.326658</td>\n",
              "      <td>0.817929</td>\n",
              "      <td>0.312557</td>\n",
              "      <td>0.818389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.309460</td>\n",
              "      <td>0.815889</td>\n",
              "      <td>0.296338</td>\n",
              "      <td>0.819269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.292144</td>\n",
              "      <td>0.814169</td>\n",
              "      <td>0.277609</td>\n",
              "      <td>0.819469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    loss_00  socre_00   loss_20  socre_20\n",
              "0  0.522875  0.769746  0.515364  0.784407\n",
              "1  0.433188  0.770006  0.433868  0.796488\n",
              "2  0.407593  0.804128  0.402953  0.802908\n",
              "3  0.388438  0.805388  0.382260  0.809349\n",
              "4  0.372958  0.815309  0.363861  0.810609\n",
              "5  0.357438  0.814269  0.346687  0.817109\n",
              "6  0.343040  0.818689  0.329133  0.817189\n",
              "7  0.326658  0.817929  0.312557  0.818389\n",
              "8  0.309460  0.815889  0.296338  0.819269\n",
              "9  0.292144  0.814169  0.277609  0.819469"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAEGCAYAAACuBLlKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}